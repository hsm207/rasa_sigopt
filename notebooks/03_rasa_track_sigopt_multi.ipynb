{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to use SigOpt to optimize a rasa NLU model for intent classification and entity extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import rasa.shared.data as data\n",
    "import sigopt\n",
    "from rasa.engine.recipes.recipe import Recipe\n",
    "from rasa.model_testing import test_core, test_nlu\n",
    "from rasa.model_training import (\n",
    "    DaskGraphRunner,\n",
    "    GraphTrainer,\n",
    "    LocalTrainingCache,\n",
    "    Path,\n",
    "    TrainingResult,\n",
    "    _create_model_storage,\n",
    "    _determine_model_name,\n",
    ")\n",
    "from rasa.shared.importers.autoconfig import TrainingType\n",
    "from rasa.shared.importers.importer import TrainingDataImporter\n",
    "import json\n",
    "import time\n",
    "import asyncio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup sigopt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sigopt config --api-token $SIGOPT_API_TOKEN --enable-log-collection --enable-cell-tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sigopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move to the bot's root directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../bot_sigopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paths to rasa configs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"config.yml\"\n",
    "training_files = \"data\"\n",
    "validation_files = \"tests\"\n",
    "domain = \"domain.yml\"\n",
    "models = \"models\"\n",
    "test_results = \"results\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to do the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, file_importer, output_path=\"models\", training_type=TrainingType.BOTH):\n",
    "    recipe = Recipe.recipe_for_name(config.get(\"recipe\"))\n",
    "    model_configuration = recipe.graph_config_for_recipe(\n",
    "        config,\n",
    "        cli_parameters={},\n",
    "        training_type=training_type,\n",
    "    )\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as temp_model_dir:\n",
    "        model_storage = _create_model_storage(\n",
    "            is_finetuning=False,\n",
    "            model_to_finetune=None,\n",
    "            temp_model_dir=Path(temp_model_dir),\n",
    "        )\n",
    "        cache = LocalTrainingCache()\n",
    "        trainer = GraphTrainer(model_storage, cache, DaskGraphRunner)\n",
    "\n",
    "        model_name = _determine_model_name(\n",
    "            fixed_model_name=None, training_type=training_type\n",
    "        )\n",
    "\n",
    "        full_model_path = Path(output_path, model_name)\n",
    "\n",
    "        trainer.train(\n",
    "            model_configuration,\n",
    "            file_importer,\n",
    "            full_model_path,\n",
    "            force_retraining=False,\n",
    "            is_finetuning=False,\n",
    "        )\n",
    "\n",
    "        return TrainingResult(str(full_model_path), 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to extract some metrics from the test: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metric(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        metrics = json.load(f)\n",
    "\n",
    "    return metrics[\"weighted avg\"][\"f1-score\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to get the config for the DIET Classifier from a config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_diet_config = lambda config: [\n",
    "    component\n",
    "    for component in config[\"pipeline\"]\n",
    "    if component[\"name\"] == \"DIETClassifier\"\n",
    "][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that will train and evaluate and nlu model given the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def train_and_evaluate_nlu_model(config, file_importer, validation_path):\n",
    "    start_time = time.time()\n",
    "\n",
    "    nlu_training_results = train(config, file_importer, training_type=TrainingType.NLU)\n",
    "\n",
    "    model_path = nlu_training_results.model\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as temp_results_dir:\n",
    "        await test_nlu(\n",
    "            model=model_path,\n",
    "            nlu_data=validation_path,\n",
    "            output_directory=temp_results_dir,\n",
    "            additional_arguments={},\n",
    "        )\n",
    "\n",
    "        f1_intent = extract_metric(f\"{temp_results_dir}/intent_report.json\")\n",
    "        f1_entity = extract_metric(f\"{temp_results_dir}/DIETClassifier_report.json\")\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    return {\n",
    "        \"f1_intent\": f1_intent,\n",
    "        \"f1_entity\": f1_entity,\n",
    "        \"elapsed_time\": end_time - start_time,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to instrument the `train_and_evaluate_nlu_model` function in sigopt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_and_track_in_sigopt():\n",
    "    sigopt.log_dataset(\"Ask Ubuntu Corpus\")\n",
    "    sigopt.log_model(\"Default NLU Pipeline\")\n",
    "    file_importer = TrainingDataImporter.load_from_config(\n",
    "        config, domain, training_files\n",
    "    )\n",
    "\n",
    "    all_config = file_importer.get_config()\n",
    "    diet_config = get_diet_config(all_config)\n",
    "\n",
    "    sigopt.params.setdefault(\"epochs\")\n",
    "    sigopt.params.setdefault(\"embedding_dimension\")\n",
    "    sigopt.params.setdefault(\"number_of_transformer_layers\")\n",
    "    sigopt.params.setdefault(\"transformer_size\")\n",
    "\n",
    "    diet_config[\"epochs\"] = sigopt.params.epochs\n",
    "    diet_config[\"embedding_dimension\"] = 20\n",
    "    diet_config[\"number_of_transformer_layers\"] = 2\n",
    "    diet_config[\"transformer_size\"] = 256\n",
    "\n",
    "    results = await train_and_evaluate_nlu_model(all_config, file_importer, \"tests\")\n",
    "\n",
    "    sigopt.log_metric(name=\"f1-score (intent)\", value=results[\"f1_intent\"])\n",
    "    sigopt.log_metric(name=\"f1-score (entity)\", value=results[\"f1_entity\"])\n",
    "    sigopt.log_metric(name=\"total time (s)\", value=results[\"elapsed_time\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the experiment configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigopt.create_experiment(name = 'Active Search',\n",
    "#                          parameters = [\n",
    "#         dict(name='hidden_layer_size', type='int', bounds= dict(min=32, max=512)),\n",
    "#         dict(name='activation_function', type='categorical', categorical_values=[\"tanh\", \"relu\"]),\n",
    "#     ],\n",
    "#                          metrics = [\n",
    "#                dict(name='holdout accuracy', strategy=\"constraint\", threshold=0.85, objective='maximize'),\n",
    "#                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%experiment\n",
    "{\n",
    "    'name': 'NLU Optimization on Ask Ubuntu Corpus - Multi Obj',\n",
    "    'type': 'offline',\n",
    "    'metrics': [\n",
    "        {\n",
    "            'name': 'f1-score (intent)',\n",
    "            'strategy': 'constraint',\n",
    "            'objective': 'maximize',\n",
    "            'threshold': 0.85\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            'name': 'f1-score (entity)',\n",
    "            'strategy': 'constraint',\n",
    "            'objective': 'maximize',\n",
    "            'threshold': 0.85\n",
    "        }\n",
    "    ],\n",
    "    'parameters': [\n",
    "        {\n",
    "            'name': 'epochs',\n",
    "            'type': 'int',\n",
    "            'bounds': {'min': 1, 'max': 1000}\n",
    "        },\n",
    "        {\n",
    "            'name': 'embedding_dimension',\n",
    "            'type': 'int',\n",
    "            'bounds': {'min': 16, 'max': 1024}\n",
    "        },\n",
    "        {\n",
    "            'name': 'number_of_transformer_layers',\n",
    "            'type': 'int',\n",
    "            'bounds': {'min': 0, 'max': 8}\n",
    "        },\n",
    "        {\n",
    "            'name': 'transformer_size',\n",
    "            'type': 'int',\n",
    "            'bounds': {'min': 16, 'max': 1024}\n",
    "        },\n",
    "    ],\n",
    "    'budget': 50,\n",
    "    'parallel_bandwidth': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%optimize Intent Classification and Entity Extraction Optimization\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(run_and_track_in_sigopt())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23393d2575091a37cff0d0e9e7479591a295495b26c3b2ebf9b64da572e02d85"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
